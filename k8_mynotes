https://docs.google.com/document/d/1wbCdqC2sINSL2787xga6y8o6_s-Wko6TlGd_UZJp0hA/edit

Installations
------------------

For installations 
On master
sudo kubeadm init 
sudo mkdir $HOME/.kube/
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
sudo kubectl get nodes

for checking all the nodes status --> kubectl get nodes , if they are not in ready state they have to get connected by the network


 kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"   --> network 
 kubectl get pods -n kube-system
 kubectl get nodes

Ref : https://www.weave.works/docs/net/latest/kubernetes/kube-addon/ 

1. to get all the namespaces
-----------------------------

kubectl get namespaces

NAME              STATUS   AGE
default           Active   14m
kube-node-lease   Active   15m
kube-public       Active   15m
kube-system       Active   15m

for getting individual pods in every namespaces --> kubectl get pods -n <namespace name>

2. for checking the token list and command to  join the node with the master again
---------------------------------------------------------------------------------------

a. kubeadm token list

labsuser@ip-172-31-30-183:~$ kubeadm token list
TOKEN                     TTL         EXPIRES                USAGES                   DESCRIPTION                                                EXTRA GROUPS
5297vm.a9hv6hw4l1j4k5lr   23h         2020-12-07T09:43:17Z   authentication,signing   The default bootstrap token generated by 'kubeadm init'.   system:bootstrappers:kubeadm:default-node-token

b.  kubeadm token create --print-join-command

labsuser@ip-172-31-30-183:~$ kubeadm token create --print-join-command
W1206 10:30:13.723111   21012 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
kubeadm join 172.31.30.183:6443 --token jcae35.nzbptat4pd8p0ni5     --discovery-token-ca-cert-hash sha256:7c57d0205c1a0991e992f5011c957561acdc49914a0f52f6eb1a94b58240fe70 


3. kubectl explain pods  --> give the information of how pods can be used and their specifications how they need to be implememted
---------------------------------------------------------------------------------------------------------------------------------------

kubectl get pods -o wide   

labsuser@ip-172-31-30-183:~$ kubectl get pods -o wide
NAME          READY   STATUS    RESTARTS   AGE    IP          NODE               NOMINATED NODE   READINESS GATES
myfirstpod1   1/1     Running   0          7m6s   10.38.0.2   ip-172-31-16-229   <none>           <none>


4. kubectl describe, explain, logs
------------------------------------------------------------ 

   kubectl describe pod <podname>
   kubectl describe service <service name>
   kubectl explain pods
   kubectl explain service
   

Name:         myfirstpod1
Namespace:    default
Priority:     0
Node:         ip-172-31-16-229/172.31.16.229
Start Time:   Sun, 06 Dec 2020 10:39:46 +0000
Labels:       mycka=simplilearn
Annotations:  <none>
Status:       Running
IP:           10.38.0.2
IPs:
  IP:  10.38.0.2
Containers:
  mycontainer:
    Container ID:   docker://b11eb13765a2641f937841667a667b4217995363aa80755a5ce5d3dba898f2d6
    Image:          docker.io/httpd
    Image ID:       docker-pullable://httpd@sha256:fddc534b7f6bb6197855be559244adb11907d569aae1283db8e6ce8bb8f6f456
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Sun, 06 Dec 2020 10:39:53 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-xdcsq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-xdcsq:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-xdcsq
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age    From                       Message
  ----    ------     ----   ----                       -------
  Normal  Scheduled  8m6s   default-scheduler          Successfully assigned default/myfirstpod1 to ip-172-31-16-229
  Normal  Pulling    8m5s   kubelet, ip-172-31-16-229  Pulling image "docker.io/httpd"
  Normal  Pulled     8m     kubelet, ip-172-31-16-229  Successfully pulled image "docker.io/httpd"
  Normal  Created    7m59s  kubelet, ip-172-31-16-229  Created container mycontainer
  Normal  Started    7m59s  kubelet, ip-172-31-16-229  Started container mycontainer
  
  
logs --> kubectl logs myfirstpod1

AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.38.0.2. Set the 'ServerName' directive globally to suppress this message
AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.38.0.2. Set the 'ServerName' directive globally to suppress this message
[Sun Dec 06 10:39:53.723916 2020] [mpm_event:notice] [pid 1:tid 140313633793152] AH00489: Apache/2.4.46 (Unix) configured -- resuming normal operations
[Sun Dec 06 10:39:53.724063 2020] [core:notice] [pid 1:tid 140313633793152] AH00094: Command line: 'httpd -D FOREGROUND'

5. kubectl get events -w or kubectl get events --watch ---> will give the output for the command we give
-------------------------------------------------------------------------------------------------------------

labsuser@ip-172-31-30-183:~$ kubectl get events --watch  (or) kubectl get events -w

LAST SEEN   TYPE     REASON      OBJECT            MESSAGE
27m         Normal   Scheduled   pod/myfirstpod1   Successfully assigned default/myfirstpod1 to ip-172-31-16-229
27m         Normal   Pulling     pod/myfirstpod1   Pulling image "docker.io/httpd"
27m         Normal   Pulled      pod/myfirstpod1   Successfully pulled image "docker.io/httpd"
27m         Normal   Created     pod/myfirstpod1   Created container mycontainer
27m         Normal   Started     pod/myfirstpod1   Started container mycontainer


6. creating a pod and svc and checking the service
----------------------------------------------------------

vi pod.yml

apiVersion: v1
kind: Pod
metadata:
  name: myfirstpod1
  labels:
    mycka: simplilearn
spec:
  containers:
  - name: mycontainer
    image: docker.io/httpd
    ports:
    - containerPort: 80
	
kubectl create -f pod.yml

kubectl get pods and ; we have label called simplilearn for the pod , u can check as below command

kubectl get pods --show-labels


vi svc.yml

kind: Service
apiVersion: v1
metadata:
  name: myservice
spec:
  selector:
      mycka: simplilearn
  ports:
    - protocol: TCP
      port: 8081
      targetPort: 80
	  
kubectl create -f svc.yml
service/myservice got created 

labsuser@ip-172-31-30-183:~$ kubectl get svc

NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP    90m
myservice    ClusterIP   10.103.194.159   <none>        8081/TCP   4m50s
labsuser@ip-172-31-30-183:~$ curl 10.103.194.159:8081
<html><body><h1>It works!</h1></body></html>


7. Creating sample text inside the pod and checking
---------------------------------------------------------

absuser@ip-172-31-30-183:~$ kubectl exec -it myfirstpod1 bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead.
root@myfirstpod1:/usr/local/apache2# echo "this is srinias from kubernetes" >> htdocs/index.html 
root@myfirstpod1:/usr/local/apache2# exit
exit

labsuser@ip-172-31-30-183:~$ kubectl get svc
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP    100m
myservice    ClusterIP   10.103.194.159   <none>        8081/TCP   14m

labsuser@ip-172-31-30-183:~$ curl 10.103.194.159:8081
<html><body><h1>It works!</h1></body></html>
hello from pod1
this is srinias from kubernetes

---------------------------------------------------------------------

8. deleting kinds

bsuser@ip-172-31-30-183:~$ kubectl delete deployments test
deployment.apps "test" deleted

kubectl delete pods <podname>
kubectl delete service <service name>


create a kind deployment by using the yaml file generated from the dry run command

 kubectl create deployment mydep --image=docker.io/httpd --dry-run -o yaml > httpd.yaml
  
  



====================================================================

kubectl get pods -n kube-system

export advertise_url=https://172.31.30.183:2379
 echo $advertise_url

kubectl exec -it -n kube-system etcd-ip-172-31-30-183 -- sh -c "ETCDCTL_API=3 etcdctl --endpoints $advertise_url --cacert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/server.key --cert /etc/kubernetes/pki/etcd/server.crt snapshot save test1.db"

kubectl cp etcd-ip-172-31-30-183:/test1.db /tmp/test1.db -n kube-system

kubectl exec -it -n kube-system etcd-ip-172-31-30-183 -- sh -c "ETCDCTL_API=3 etcdctl --endpoints $advertise_url --cacert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/server.key --cert /etc/kubernetes/pki/etcd/server.crt get \"\" --prefix=true -w json" > etcd.json
ls
cat etcd.json 
for k in $(cat etcd.json | jq '.kvs[].key' | cut -d '"' -f2); do echo $k | base64 --decode; echo; done
sudo apt  install jq
for k in $(cat etcd.json | jq '.kvs[].key' | cut -d '"' -f2); do echo $k | base64 --decode; echo; done
for k in $(cat etcd.json | jq '.kvs[].value' | cut -d '"' -f2); do echo $k | base64 --decode; echo; done
-



======================================================

CLass A and CLass B networks -- check this

k8 suppports 250 pods per node, conditionally it must have resources.

Sizing Information
====================

kubernetes.io/docs/setup/best-practices/cluster-large/  -------> link

No more than 100 pods per node
No more than 5000 nodes
No more than 150000 total pods
No more than 300000 total containers



======================

Static Pod
------------


Static POD 
cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
# Note: This dropin only works with kubeadm and kubelet v1.11+
[Service]
Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"
Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"
# This is a file that "kubeadm init" and "kubeadm join" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use
# the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.
EnvironmentFile=-/etc/default/kubelet
ExecStart=
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS --pod-manifest-path=/test/


=======================

labsuser@ip-172-31-30-183:~$ cat multi.yml 
apiVersion: v1
kind: Pod
metadata:
  name: mc1
spec:
  volumes:
  - name: html
    emptyDir: {}
  containers:
  - name: 1st
    image: nginx
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
  - name: 2nd
    image: debian
    volumeMounts:
    - name: html
      mountPath: /html
    command: ["/bin/sh", "-c"]
    args:
      - while true; do
          date >> /html/index.html;
          sleep 1;
        done

kubectl logs -c 1st mc1  --> 1st is name of the first container in pod and mc1 is the name of the pod

kubectl logs -c 2nd mc1  -->  2nd is the name of the second container and mc1 is the name of the pod as usual

labsuser@ip-172-31-30-183:~$ kubectl logs -c 1st mc1
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: Getting the checksum of /etc/nginx/conf.d/default.conf
10-listen-on-ipv6-by-default.sh: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Configuration complete; ready for start up

=================================




NAME                    READY   STATUS             RESTARTS   AGE     IP          NODE               NOMINATED NODE   READINESS GATES
liveness-exec           0/1     CrashLoopBackOff   19         57m     10.32.0.5   ip-172-31-17-87    <none>           <none>

check this CrashLoobBackOff


==========

Upgrade

sudo kubeadm upgrade plan
sudo apt update
apt-cache madison kubeadm


   101  sudo apt-get update && sudo apt-get install -y kubeadm=1.19.5-00 --allow-change-held-packages
  102  sudo kubeadm upgrade apply v1.19.5



=================================================

Jan 26th, 2021
---------------

Clusterrolebinding

kubectl get sa -n kubernetes-dashboard 

Application Lifecycle Management
---------------------------------

Update deployment rolling using kubectl command and by editing the deployment file.

Check rollback status, pause, and resume rollback 

Create a pod with arguments and commands

Create a Configmap file

scale the applications up and down

Demonstrate the use of initcontainers and self-healing

===================================================== -------> learn abt these

Examples:
  # Create a deployment named my-dep that runs the busybox image.
  kubectl create deployment my-dep --image=busybox
  
  # Create a deployment with command
  kubectl create deployment my-dep --image=busybox -- date
  
  # Create a deployment named my-dep that runs the nginx image with 3 replicas.
  kubectl create deployment my-dep --image=nginx --replicas=3
  
  # Create a deployment named my-dep that runs the busybox image and expose port 5701.
  kubectl create deployment my-dep --image=busybox --port=5701
  
kubectl create deployment my-dep --image=docker.io/httpd


there are two types of deployment --> rolling update and recreate 

edit the deployment file
---------------------------

kubectl edit deployment my-dep

check how to edit the deployment file

kubectl create deployment mydep --image=docker.io/httpd


RollingUpdate --> rolling will not have downtime, recreate will have the downtime

after editing check the running logs
------------------------------------



labsuser@ip-172-31-8-75:~$ kubectl get pod -w
NAME                     READY   STATUS              RESTARTS   AGE
mydep-68c6f59b46-97x65   1/1     Running             0          3m57s
mydep-787bff7df7-mddkp   0/1     Terminating         0          77s
mydep-7965c47bfb-g6kck   0/1     ContainerCreating   0          1s
mydep-787bff7df7-mddkp   0/1     Terminating         0          78s
mydep-787bff7df7-mddkp   0/1     Terminating         0          78s
mydep-7965c47bfb-g6kck   1/1     Running             0          10s
mydep-68c6f59b46-97x65   1/1     Terminating         0          4m6s
mydep-68c6f59b46-97x65   0/1     Terminating         0          4m8s
mydep-68c6f59b46-97x65   0/1     Terminating         0          4m14s
mydep-68c6f59b46-97x65   0/1     Terminating         0          4m14s


Imagepullpolicy --> always, if notpresent , never

Checking the rollout status of deployment mydep

labsuser@ip-172-31-8-75:~$ kubectl rollout history deployment/mydep
deployment.apps/mydep 
REVISION  CHANGE-CAUSE
1         kubectl run mydep --image=ghost:0.9 --record=true --dry-run=true --output=yaml  

here (kubectl run mydep --image=ghost:0.9 --record=true --dry-run=true --output=yaml  ) this is used as a annotations as a change-cause
REVISION 1 is the , it has done only 1 change

Changing the image version to 0.11 --> kubectl set image deployment/mydep mydep=ghost:0.11 --record

kubectl set image deployment/mydep mydep=ghost:0.11 --record
deployment.apps/mydep image updated

labsuser@ip-172-31-8-75:~$ kubectl rollout history deployment/mydep
deployment.apps/mydep 
REVISION  CHANGE-CAUSE
1         kubectl run mydep --image=ghost:0.9 --record=true --dry-run=true --output=yaml
2         kubectl set image deployment/mydep mydep=ghost:0.11 --record=true
3         kubectl set image deployment/mydep mydep=ghost:0.12 --record=true


to pause the rollout process (to avoid accidental update)--> kubectl rollout pause deployment/mydep --> if you execute any rollout it wont execute, it will be on hold untill it is resumed

kubectl rollout resume deployment/mydep   

kubectl rollout undo deployment/mydep --to-revision=1

========================================================================
========================================================================

undo command to jump to particular revision in rollout

kubectl rollout undo deployment/mydep --to-revision=1


labsuser@ip-172-31-8-75:~$ kubectl describe deployment mydep
Name:                   mydep
Namespace:              default
CreationTimestamp:      Tue, 26 Jan 2021 12:20:30 +0000
Labels:                 run=mydep
Annotations:            deployment.kubernetes.io/revision: 5
                        kubernetes.io/change-cause: kubectl set image deployment/mydep mydep=ghost:0.13 --record=true
Selector:               run=mydep
Replicas:               1 desired | 1 updated | 2 total | 1 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  run=mydep
  Containers:
   mydep:
    Image:        ghost:0.13
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status   Reason
  ----           ------   ------
  Available      True     MinimumReplicasAvailable
  Progressing    Unknown  DeploymentPaused             --------------------------------> when it is paused, you can check the status using describe
OldReplicaSets:  mydep-567c7c78 (1/1 replicas created)
NewReplicaSet:   mydep-79dd9fc9d9 (1/1 replicas created)
Events:
  Type    Reason             Age                  From                   Message
  ----    ------             ----                 ----                   -------
  Normal  ScalingReplicaSet  12m                  deployment-controller  Scaled up replica set mydep-6d5bc8767f to 1
  Normal  ScalingReplicaSet  11m                  deployment-controller  Scaled down replica set mydep-567c7c78 to 0
  Normal  ScalingReplicaSet  10m                  deployment-controller  Scaled up replica set mydep-859f4fc794 to 1
  Normal  ScalingReplicaSet  8m21s (x2 over 17m)  deployment-controller  Scaled up replica set mydep-567c7c78 to 1
  Normal  ScalingReplicaSet  8m21s                deployment-controller  Scaled down replica set mydep-859f4fc794 to 0
  Normal  ScalingReplicaSet  8m19s                deployment-controller  Scaled down replica set mydep-6d5bc8767f to 0
  Normal  ScalingReplicaSet  4m7s                 deployment-controller  Scaled up replica set mydep-79dd9fc9d9 to 1

logging commands
============================================================================================================
============================================================================================================

kubectl get events

kubectl logs podname -f

kubectl get pod/pods -w

apiVersion: v1
kind: Pod
metadata:
  name: apache3
  labels:
    mycka: simplilearn
spec:
  containers:
  - name: mycontainer
    image: docker.io/ubuntu
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello; sleep 1;done"]

    ports:
    - containerPort: 80
	
		
IMPORTANT FILES
=====================================
=====================================

sudo find / -name *.env
/var/lib/kubelet/kubeadm-flags.env

One of the student had issue, coredns is not working remove this flag on both master and node ""--network-plugin=cni""
====================================================
labsuser@ip-172-31-8-75:~$ sudo cat /var/lib/kubelet/kubeadm-flags.env
KUBELET_KUBEADM_ARGS="--network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.2"

SECRETS  -------------> used for setting the sensitive information

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: mydb
  name: mydb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mydb
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: mydb
    spec:
      containers:
      - image: docker.io/mysql:5.6
        name: mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: centos
        resources: {}
status: {}


Create deployemnt --> kubectl create -f mydb.yml

labsuser@ip-172-31-8-75:~$ kubectl create secret --help 
Create a secret using specified subcommand.

Available Commands:
  docker-registry Create a secret for use with a Docker registry
  generic         Create a secret from a local file, directory or literal value
  tls             Create a TLS secret
  
  
  labsuser@ip-172-31-8-75:~$ kubectl create secret generic mysecret --from-literal='dbpass'='centos'
secret/mysecret created
labsuser@ip-172-31-8-75:~$ kubectl edit secret mysecret

here if you edit the secret using --> kubectl edit secret mysecret --> password will be encrypted

apiVersion: v1
data:
  dbpass: Y2VudG9z
kind: Secret
metadata:
  creationTimestamp: "2021-01-29T17:49:37Z"
  name: mysecret
  namespace: default
  resourceVersion: "30849"
  selfLink: /api/v1/namespaces/default/secrets/mysecret
  uid: 871aa33f-47bf-4710-a837-8690caca18ae
type: Opaque

labsuser@ip-172-31-8-75:~$ kubectl create secret generic mynewsecret --from-literal='mypass'='centos'
secret/mynewsecret created
labsuser@ip-172-31-8-75:~$ kubectl get secret
NAME                  TYPE                                  DATA   AGE
default-token-xs9pm   kubernetes.io/service-account-token   3      3d7h
mynewsecret           Opaque                                1      10s
mysecret              Opaque                                1      35s

Here, if you list kubectl get secret 

labsuser@ip-172-31-8-75:~$ kubectl create secret generic mysecret2 --from-literal='mypass'='centos' --from-literal='mypass1'='ubuntu'
secret/mysecret2 created
labsuser@ip-172-31-8-75:~$ kubectl get secret
NAME                  TYPE                                  DATA   AGE
default-token-xs9pm   kubernetes.io/service-account-token   3      3d7h
mynewsecret           Opaque                                1      62s
mysecret              Opaque                                1      87s
mysecret2             Opaque                                2      11s

===========================================================================

sudo kubectl edit deployment mydb

        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              key: mypass
              name: mysecret


after that , kubectl get pods

labsuser@ip-172-31-8-75:~$ kubectl get pods
NAME                     READY   STATUS             RESTARTS   AGE
apache3                  1/1     Running            1          3d4h
mydb-6b6cc656d9-llrwr    1/1     Running            0          4m9s

kubectl exec -it  mydb-6b6cc656d9-llrwr  bash

then u will get root prompt --> mysql -uroot -pcentos --> just to verify the given password in secret is working fine.



CONFIGMAP          --> used to map the non sensitive information like the username, form name, mail id etc....
=================

used to pass varaibales or arguments for pod (in keyvalue form) is called configmap

Cat cmap.yml
kind: ConfigMap 
apiVersion: v1 
metadata:
  name: example-configmap 
data:
  # Configuration values can be set as key-value properties
  database: mongodb 							--> used to map (here we are mapping 2 values)
  database_uri: mongodb://localhost:27017		--> used to map


kind: Pod 
apiVersion: v1 
metadata:
  name: pod-env-var 
spec:
  containers:
    - name: env-var-configmap
      image: nginx:1.7.9 
      envFrom:
        - configMapRef:
            name: example-configmap




labsuser@ip-172-31-8-75:~$ kubectl exec -it pod-env-var bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
root@pod-env-var:/# env
HOSTNAME=pod-env-var
KUBERNETES_PORT=tcp://10.96.0.1:443
KUBERNETES_PORT_443_TCP_PORT=443
TERM=xterm
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_HOST=10.96.0.1
database_uri=mongodb://localhost:27017     ------------------------> mapped as an env here where it is used from the configmap
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
database=mongodb                 			----------------------> mapped as an env here
PWD=/
NGINX_VERSION=1.7.9-1~wheezy
SHLVL=1
HOME=/root
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
_=/usr/bin/env


=====================================================================

Using ConfigMap for  file in the pod below



apiVersion: v1
kind: Pod
metadata:
  name: testconfig
spec:
  containers:
    - name: test
      image: docker.io/httpd
      volumeMounts:
      - name: config-volume
        mountPath: /tmp/myenvs/  -----------> path , in this path we are mapping the example-config (config map) , here all the config will be present in example-configmap 
  volumes:
    - name: config-volume
      configMap:
        name: example-configmap   ------------> this is the configmap name
  restartPolicy: Never
  
  example-configmap content
  ============================
  apiVersion: v1
data:
  database: mongodb
  database_uri: mongodb://localhost:27017
kind: ConfigMap
metadata:
  creationTimestamp: "2021-01-29T18:21:48Z"
  name: example-configmap
  namespace: default
  resourceVersion: "35546"
  selfLink: /api/v1/namespaces/default/configmaps/example-configmap
  uid: 0b6266a3-e92a-4a40-bacc-2b75a1869fea
  
  ------------------------------------------------
  
  if u go inside the pods after 
  
  labsuser@ip-172-31-8-75:~$ kubectl get pods
NAME                     READY   STATUS             RESTARTS   AGE
apache3                  1/1     Running            1          3d5h
mydb-6b6cc656d9-llrwr    1/1     Running            0          44m
mydep-567c7c78-7bz84     1/1     Running            1          3d6h
mydep-79dd9fc9d9-4kvzl   0/1     ImagePullBackOff   0          3d6h
pod-env-var              1/1     Running            0          28m
testconfig               1/1     Running            0          5m21s
labsuser@ip-172-31-8-75:~$ kubectl exec -it testconfig bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
root@testconfig:/usr/local/apache2# cd /tmp/myenvs/
root@testconfig:/tmp/myenvs# ls -lrt     
total 0
lrwxrwxrwx 1 root root 19 Jan 29 18:47 database_uri -> ..data/database_uri
lrwxrwxrwx 1 root root 15 Jan 29 18:47 database -> ..data/database
root@testconfig:/tmp/myenvs# cat database 
mongodbroot@testconfig:/tmp/myenvs# 




=====================

kubectl create deployment test11 --image=nginx:1.7.9 -o yaml > test11.yml

cat pod2.yml
kind: Pod 
apiVersion: v1 
metadata:
  name: pod-env12
spec:
  containers:
    - name: env-var-configmap
      image: nginx:1.7.9 
      env:
        - name: testenv
          valueFrom: 
           configMapKeyRef:
             name: example-configmap
             key: database

=============================================
test11.yml

apiVersion: apps/v1
kind: Deployment
metadata:
metadata:
  creationTimestamp: null
  labels:
    app: test11
  name: test11
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test11
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: test11
    spec:
      containers:
      - image: nginx:1.7.9
        name: nginx
        env:
        - name: testenv
          valueFrom:
           configMapKeyRef:
             name: example-configmap
             key: database
        resources: {}
status: {}

kubectl create -f test11.yml

kubectl exec -it test11-677b8d6f69-6mk2t bash --> then give env (here we have added a deployment nginx to the configmap -> example-configmap)

=====================================

If you create a deployemnt with "n" number of replicas, then replica set will be created as per the deployment,
we cant change the replica set and edit replicas, since this kind was not created separately, it was created with the deployemnt only.



MULTI CONTAINER PODS
=====================================================================================================
=====================================================================================================
=====================================================================================================

these can be used when they the pods are dependent (ex -> side car) , 

The pod comprises of application that consists of multiple tightly coupled containers.

Best example of multi container pod is waeve pod


labsuser@ip-172-31-8-75:~$ sudo docker images | grep -i weave
weaveworks/weave-npc                 2.8.1               7f92d556d4ff        6 days ago          39.3MB
weaveworks/weave-kube                2.8.1               df29c0a4002c        6 days ago          89MB


to check logs for 2 containers in one pod is below

kubectl logs weave-net-smjsl -c weave-npc


kubectl logs weave-net-smjsl -c weave-npc -n kube-system

kubectl logs weave-net-smjsl -c weave -n kube-system


Containers:
  weave:
    Container ID:  docker://e3049ff1aa6a697ba03097af3a05ebc73c2541e8e8aab0fc148d6aace693207c
    Image:         docker.io/weaveworks/weave-kube:2.8.1
    Image ID:      docker-pullable://weaveworks/weave-kube@sha256:d797338e7beb17222e10757b71400d8471bdbd9be13b5da38ce2ebf597fb4e63
    
	weave-npc:
    Container ID:   docker://0e49931f046bd7ef74e9a4ccd2866fe25f7b1566e62e1caa9d7f3404205062b3
    Image:          docker.io/weaveworks/weave-npc:2.8.1
    Image ID:       docker-pullable://weaveworks/weave-npc@sha256:38d3e30a97a2260558f8deb0fc4c079442f7347f27c86660dbfc8ca91674f14c

 
 
 
 multipod
 ---------
 to enter into the pod in multipod
 
 kubectl exec -it -c 1st         mc1      bash
                     continer    podname 
					 
apiVersion: v1
kind: Pod
metadata:
  name: mc1
spec:
  volumes:
  - name: html
    emptyDir: {}
  containers:
  - name: 1st
    image: nginx
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
  - name: 2nd
    image: debian
    volumeMounts:
    - name: html
      mountPath: /html
    command: ["/bin/sh", "-c"]
    args:
      - while true; do
        echo "hello"  >> /html/index.html;
          sleep 1;
        done

--------

Init container
================
Init container will check the pre-requisite required for the pod to create , if it does not matches, the pod wont get created.

apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:	 -----------------------------------------> first container
  - name: myapp-container
    image: docker.io/httpd
  initContainers: ----------------------------------------> init containers, here in this pod file, initcontainers will execute first, check the condition
  - name: init-myservice
    image: busybox
    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for  myservice; sleep 2; done;']
	

kubectl get pods -w      

myapp-pod                 0/1     Init:0/1           0          69s

kubectl logs myapp-pod -c init-myservice

waiting for myservice
Server:         10.96.0.10
Address:        10.96.0.10:53

** server can't find myservice.default.svc.cluster.local: NXDOMAIN

*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer
*** Can't find myservice.us-west-2.compute.internal: No answer
*** Can't find myservice.default.svc.cluster.local: No answer
*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer
*** Can't find myservice.us-west-2.compute.internal: No answer

waiting for myservice

Once service is created, it automatically crates the pod
=============================

pod with 2 initcontainers

cat init2.yml 
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: docker.io/httpd
  initContainers:   
  - name: init-myservice     ---------------> first this will execute and check for myservice service, if it exists wil go to next init-mydb 
    image: busybox
    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for  myservice; sleep 2; done;']
  - name: init-mydb         ----------------> second it will execute and check for mydb service
    image: busybox
    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']

=====================================================================

Init containers for using the wordpress

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: mywp
  name: mywp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mywp
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: mywp
    spec:
      containers:
      - image: docker.io/wordpress
        name: worodpress
        env: 
        - name: WORDPRESS_DB_HOST
          value: mydb.mysql
        - name: WORDPRESS_DB_PASSWORD
          value: centos
	  initContainers: --------------------------------> init container for checking the pod to start , service mydb
      - name: init-myservice
        image: busybox
        command: ['sh', '-c', 'until nslookup mydb; do echo waiting for  myservice; sleep 2; done;']
        resources: {}
status: {}

SELF HEALING APPLICATIONS
==========================

We use probes here, it does vice versa job of init containers 


apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
  - name: liveness
    image: k8s.gcr.io/busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: mywp
  name: mywp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mywp
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: mywp
    spec:
      containers:
      - image: docker.io/wordpress
        name: worodpress
        env: 
        - name: WORDPRESS_DB_HOST
          value: mydb.mysql
        - name: WORDPRESS_DB_PASSWORD
          value: centos
		  args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
        resources: {}
status: {}

there are 2 types of probes --> liveness and readiness probes

example with both liveness and readiness probes are below

apiVersion: v1
kind: Pod
metadata:
  name: goproxy
  labels:
    app: goproxy
spec:
  containers:
  - name: goproxy
    image: k8s.gcr.io/goproxy:0.1
    ports:
    - containerPort: 8080
    readinessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 10
    livenessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 20
---

Lifecycle hooks ---> https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/ 

Kubernetes - dashboard

kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml

kubectl get svc -n kubernetes-dashboard

kubectl edit  svc kubernetes-dashboard -n kubernetes-dashboard  --> make it to expose as NodePort

kubectl get nodes -o wide

To get the token to login to the kubernetes-dashboard using token, there is an other option also called kube-config
------------------------------------------------------------------------------------------------------------------------
kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep kubernetes-dashboard | awk '{print $1}')

Service Account  --> is  like a user
----------------

service account is like a user, example we have a user myuser1 

Cluster Role  is like admin access 

Cluster Role Bindings = SA + Cluster Role --> cluster role bindings 

(cluster role binding ) Mapping = Admin (cluster role) + myuser1 (service account) 

TASK
=============

1.    Create a wordpress pod in a namespace called frontend and map it to the database resides into the namespace called backend 
       (dont use IP address of service, use service name) 
2.    Create a mysql pod in a namespace called backen

 



